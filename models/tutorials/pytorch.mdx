---
title: PyTorch
---
import { ColabLink } from '/snippets/en/_includes/colab-link.mdx';

<ColabLink url="https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Simple_PyTorch_Integration.ipynb" />

Use [W&B](https://wandb.ai) for machine learning experiment tracking, dataset versioning, and project collaboration.


<Frame>
    <img src="/images/tutorials/huggingface-why.png" alt="Benefits of using W&B"  />
</Frame>

This tutorial demonstrates how to integrate Weights & Biases with a PyTorch training pipeline to track experiments.

## Install, import, and log in

This section sets up the Python environment for the tutorial, including imports, reproducibility settings, and device configuration.

```python
import os
import random

import numpy as np
import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
from tqdm.auto import tqdm

# Ensure deterministic behavior
torch.backends.cudnn.deterministic = True
random.seed(hash("setting random seeds") % 2**32 - 1)
np.random.seed(hash("improves reproducibility") % 2**32 - 1)
torch.manual_seed(hash("by removing stochasticity") % 2**32 - 1)
torch.cuda.manual_seed_all(hash("so runs are repeatable") % 2**32 - 1)

# Device configuration
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# remove slow mirror from list of MNIST mirrors
torchvision.datasets.MNIST.mirrors = [mirror for mirror in torchvision.datasets.MNIST.mirrors
                                      if not mirror.startswith("http://yann.lecun.com")]
```

### Install and log in to W&B

1. Install the W&B library using `pip`.

```python
!pip install wandb onnx -Uq
```
2. Import W&B and log in.

> Note: If this is your first time using W&B, follow the link that appears to create a free account.

```
import wandb

wandb.login()
```

## Define the experiment configuration

Start by defining the configuration for your experiment. This configuration captures the hyperparameters and metadata you want to track for each run, such as training settings, dataset choice, and model architecture. Including this metadata helps distinguish and compare runs within the same project.

In the example below, only a subset of values is varied, but any part of your training setup can be included in the configuration.

```python
config = dict(
    epochs=5,
    classes=10,
    kernels=[16, 32],
    batch_size=128,
    learning_rate=0.005,
    dataset="MNIST",
    architecture="CNN")
```

## Define the training pipeline

Next, define the overall training pipeline. This pipeline follows a typical model-training workflow: 

1. Create the model and data loaders. 
2. Train the model. 
3. Evaluate its performance.

```python
def model_pipeline(hyperparameters):

    # tell wandb to get started
    with wandb.init(project="pytorch-demo", config=hyperparameters) as run:
        # access all HPs through run.config, so logging matches execution.
        config = run.config

        # make the model, data, and optimization problem
        model, train_loader, test_loader, criterion, optimizer = make(config)
        print(model)

        # and use them to train the model
        train(model, train_loader, criterion, optimizer, config)

        # and test its final performance
        test(model, test_loader)

    return model
```
Wrapping the pipeline in `wandb.init` ensures that all configuration values, metrics, and artifacts logged during training are associated with a single run.

The helper functions used here are defined in the following sections.

## Define the model, data loaders, and optimizer

```python
def make(config):
    # Make the data
    train, test = get_data(train=True), get_data(train=False)
    train_loader = make_loader(train, batch_size=config.batch_size)
    test_loader = make_loader(test, batch_size=config.batch_size)

    # Make the model
    model = ConvNet(config.kernels, config.classes).to(device)

    # Make the loss and optimizer
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(
        model.parameters(), lr=config.learning_rate)
    
    return model, train_loader, test_loader, criterion, optimizer
```

### Define the data loading and model

This section defines the data loading utilities and model architecture used by the training pipeline. These components are standard PyTorch implementations and are included here for completeness.

```python
def get_data(slice=5, train=True):
    full_dataset = torchvision.datasets.MNIST(root=".",
                                              train=train, 
                                              transform=transforms.ToTensor(),
                                              download=True)
    #  equiv to slicing with [::slice] 
    sub_dataset = torch.utils.data.Subset(
      full_dataset, indices=range(0, len(full_dataset), slice))
    
    return sub_dataset


def make_loader(dataset, batch_size):
    loader = torch.utils.data.DataLoader(dataset=dataset,
                                         batch_size=batch_size, 
                                         shuffle=True,
                                         pin_memory=True, num_workers=2)
    return loader
```

The model below uses a simple convolutional neural network suitable for the MNIST dataset.

```python
# Conventional and convolutional neural network

class ConvNet(nn.Module):
    def __init__(self, kernels, classes=10):
        super(ConvNet, self).__init__()
        
        self.layer1 = nn.Sequential(
            nn.Conv2d(1, kernels[0], kernel_size=5, stride=1, padding=2),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2))
        self.layer2 = nn.Sequential(
            nn.Conv2d(16, kernels[1], kernel_size=5, stride=1, padding=2),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2))
        self.fc = nn.Linear(7 * 7 * kernels[-1], classes)
        
    def forward(self, x):
        out = self.layer1(x)
        out = self.layer2(out)
        out = out.reshape(out.size(0), -1)
        out = self.fc(out)
        return out
```

### Define the training logic

This section explains how the model is trained. The structure is a standard PyTorch training loop: iterate over epochs and batches, run forward and backward passes, and update model parameters with the optimizer.

Nothing here is specific to W&B. The functions shown below would look the same in a script that does not use experiment tracking.

```python
def train(model, loader, criterion, optimizer, config):
    # Tell wandb to watch what the model gets up to: gradients, weights, and more.
    run = wandb.init(project="pytorch-demo", config=config)
    run.watch(model, criterion, log="all", log_freq=10)

    # Run training and track with wandb
    total_batches = len(loader) * config.epochs
    example_ct = 0  # number of examples seen
    batch_ct = 0
    for epoch in tqdm(range(config.epochs)):
        for _, (images, labels) in enumerate(loader):

            loss = train_batch(images, labels, model, optimizer, criterion)
            example_ct +=  len(images)
            batch_ct += 1

            # Report metrics every 25th batch
            if ((batch_ct + 1) % 25) == 0:
                train_log(loss, example_ct, epoch)


def train_batch(images, labels, model, optimizer, criterion):
    images, labels = images.to(device), labels.to(device)
    
    # Forward pass ➡
    outputs = model(images)
    loss = criterion(outputs, labels)
    
    # Backward pass ⬅
    optimizer.zero_grad()
    loss.backward()

    # Step with optimizer
    optimizer.step()

    return loss
```
### Log gradients and metrics with W&B

This section shows where W&B fits into the training loop. Two APIs are used:

* `run.watch()` to track gradients and model parameters

* `run.log()` to record metrics during training

Call `run.watch()` once before training starts. After that, logging metrics is as simple as passing a dictionary to run.log().

```python
run.watch(model, criterion, log="all", log_freq=10)
```
Metrics are logged during training using a helper function:

```python
def train_log(loss, example_ct, epoch):
    run.log(
        {
            "epoch": epoch,
            "loss": loss
        },
        step=example_ct
    )
```
The only difference from a typical training script is where metrics go. Instead of printing values to the terminal, you send them to W&B, where they are stored and visualized for the current run.

Using the number of examples processed as the logging step makes it easier to compare runs with different batch sizes, but you can also log by batch or by epoch depending on your needs.

### Define testing logic

After training completes, evaluate the model on held-out test data to measure its performance. This step runs the trained model in evaluation mode and reports aggregate metrics, such as accuracy.

Optionally, you can also save the trained model for later use or deployment.


## (Optional) Call `run.save()`

After evaluation, you may want to persist the model’s architecture and learned parameters. 
In this example, the model is exported to the ONNX format for portability and interoperability.

Calling `run.save()` uploads the exported file to W&B and associates it with the current run. This makes it easy to track which saved models correspond to which experiments.

For more advanced workflows, including model versioning and distribution, see the [Artifacts tools](https://www.wandb.com/artifacts) documentation.

```python
def test(model, test_loader):
    model.eval()

    with wandb.init(project="pytorch-demo") as run:
        # Run the model on some test examples
        with torch.no_grad():
            correct, total = 0, 0
            for images, labels in test_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

            print(f"Accuracy of the model on the {total} " +
                f"test images: {correct / total:%}")
            
            run.log({"test_accuracy": correct / total})

        # Save the model in the exchangeable ONNX format
        torch.onnx.export(model, images, "model.onnx")
        run.save("model.onnx")
```

### Run training and view metrics in W&B

With the pipeline defined, you can now run a fully tracked training session.

When the run starts, W&B prints links to:

*The project page, which groups all runs for this experiment.
*The run page, where metrics, logs, and files for this run are recorded.

On the run page, you can explore the following tabs:

1. Charts: View training metrics such as loss, gradients, and parameter values as they change over time.
2. System: Monitor system metrics including CPU, GPU, memory usage, and disk I/O during training.
3. Logs: See a record of anything written to standard output while the model runs.
4. Files: Access files saved during the run. After training completes, you can open model.onnx in the Netron model viewer to inspect the network architecture.

When training finishes and the wandb.init context exits, a summary of the run is also printed in the notebook output.

```python
# Build, train, and evaluate the model
model = model_pipeline(config)
```
## (Optional) Next steps

### Test hyperparameters with sweeps

We only looked at a single set of hyperparameters in this example.
But an important part of most ML workflows is iterating over
a number of hyperparameters.

You can use W&B Sweeps to automate hyperparameter testing and explore the space of possible models and optimization strategies.

Check out a [Colab notebook demonstrating hyperparameter optimization using W&B Sweeps](https://wandb.me/sweeps-colab).

Running a hyperparameter sweep with W&B is very easy. There are just 3 simple steps:

1. **Define the sweep:** We do this by creating a dictionary or a [YAML file](/models/sweeps/define-sweep-configuration/) that specifies the parameters to search through, the search strategy, the optimization metric et all.

2. **Initialize the sweep:** 
`sweep_id = wandb.sweep(sweep_config)`

3. **Run the sweep agent:** 
`wandb.agent(sweep_id, function=train)`

That's all there is to running a hyperparameter sweep.

<Frame>
    <img src="/images/tutorials/pytorch-2.png" alt="PyTorch training dashboard"  />
</Frame>

## Minimal integration example

<Frame>
    <img src="/images/tutorials/pytorch.png" alt="PyTorch and W&B integration diagram"  />
</Frame>

```python
# import the library
import wandb

# start a new experiment
with wandb.init(project="new-sota-model") as run:
 
    # capture a dictionary of hyperparameters with config
    run.config = {"learning_rate": 0.001, "epochs": 100, "batch_size": 128}

    # set up model and data
    model, dataloader = get_model(), get_data()

    # optional: track gradients
    run.watch(model)

    for batch in dataloader:
    metrics = model.training_step()
    # log metrics inside your training loop to visualize model performance
    run.log(metrics)

    # optional: save model at the end
    model.to_onnx()
    run.save("model.onnx")
```

Follow along with a [video tutorial](https://wandb.me/pytorch-video).

**Note**: Sections starting with _Step_ are all you need to integrate W&B in an existing pipeline. The rest just loads data and defines a model.

## Example Gallery

Explore examples of projects tracked and visualized with W&B in our [Gallery →](https://app.wandb.ai/gallery).

## Advanced Setup
1. [Environment variables](/platform/hosting/env-vars/): Set API keys in environment variables so you can run training on a managed cluster.
2. [Offline mode](/models/support/run_wandb_offline/): Use `dryrun` mode to train offline and sync results later.
3. [On-prem](/platform/hosting/hosting-options/self-managed): Install W&B in a private cloud or air-gapped servers in your own infrastructure. We have local installations for everyone from academics to enterprise teams.
4. [Sweeps](/models/sweeps/): Set up hyperparameter search quickly with our lightweight tool for tuning.
